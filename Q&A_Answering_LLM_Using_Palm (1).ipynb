{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install streamlit langchain langchain-community langchain-google-genai \\\n",
        "               InstructorEmbedding sentence-transformers google-generativeai pyngrok"
      ],
      "metadata": {
        "id": "3RJZHnJupKPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "google_api = \"Your_API_Key\"   # your API key\n",
        "\n",
        "# --- LLM (make sure GOOGLE_API_KEY is set in env or pass google_api_key here) ---\n",
        "google_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.0,\n",
        "    google_api_key=google_api\n",
        "    # google_api_key=os.environ.get(\"GOOGLE_API_KEY\"),  # or pass explicitly\n",
        ")\n",
        "\n",
        "# --- Embeddings (requires: pip install InstructorEmbedding sentence-transformers) ---\n",
        "instructor_embedding = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-base\")\n",
        "\n",
        "INDEX_DIR = \"faiss_index\"  # folder for FAISS index (index.faiss + index.pkl)\n",
        "CSV_PATH = \"codebasics_faqs_utf8.csv\"  # make sure you've re-saved as UTF-8\n",
        "\n",
        "def create_vectordb():\n",
        "    loader = CSVLoader(file_path=CSV_PATH, source_column=\"prompt\")\n",
        "    docs = loader.load()\n",
        "    vectordb = FAISS.from_documents(documents=docs, embedding=instructor_embedding)\n",
        "    vectordb.save_local(INDEX_DIR)\n",
        "\n",
        "def ensure_index():\n",
        "    if not os.path.exists(os.path.join(INDEX_DIR, \"index.faiss\")):\n",
        "        create_vectordb()  # builds and saves ./faiss_index\n",
        "\n",
        "def get_qa_chain():\n",
        "\n",
        "    vectordb = FAISS.load_local(\n",
        "    INDEX_DIR,                 # or vector_db_path\n",
        "    instructor_embedding,\n",
        "    allow_dangerous_deserialization=True   # <-- required\n",
        ")\n",
        "\n",
        "    retriever = vectordb.as_retriever(\n",
        "        search_type=\"similarity\",           # start simple\n",
        "        search_kwargs={\"k\": 4}              # tune later; threshold can hide all docs\n",
        "    )\n",
        "\n",
        "    prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "In the answer, reuse as much text as possible from the \"response\" section of the source context.\n",
        "If the answer is not found in the context, say \"I don't know.\"\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=google_llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        input_key=\"query\",\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ensure_index()  # <-- builds index once if missing\n",
        "    qa = get_qa_chain()\n",
        "    result = qa({\"query\": \"Should I learn PowerBI or Tableau?\"})\n",
        "    print(result[\"result\"])\n",
        "# ---------------- Streamlit UI ----------------\n",
        "st.title(\"Codebasics Q&A ðŸŒ±\")\n",
        "if st.button(\"Create Knowledgebase\"):\n",
        "    try:\n",
        "        create_vectordb()\n",
        "        st.success(f\"Vector index saved to ./{INDEX_DIR}\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to create index: {e}\")\n",
        "\n",
        "question = st.text_input(\"Question:\")\n",
        "if question:\n",
        "    try:\n",
        "        chain = get_qa_chain()\n",
        "        # IMPORTANT: pass dict because input_key=\"query\"\n",
        "        response = chain({\"query\": question})\n",
        "        st.header(\"Answer\")\n",
        "        st.write(response.get(\"result\", \"\"))\n",
        "\n",
        "        # Optional: show sources\n",
        "        src_docs = response.get(\"source_documents\") or []\n",
        "        if src_docs:\n",
        "            st.subheader(\"Sources\")\n",
        "            for i, d in enumerate(src_docs, 1):\n",
        "                st.write(f\"{i}. {d.metadata.get('source', 'N/A')}\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Q&A failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vukm9xqaLqPn",
        "outputId": "782f110d-1c04-432d-f286-6c99cc46f7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kill any previous server on 8501, then start Streamlit\n",
        "!fuser -k 8501/tcp >/dev/null 2>&1 || true\n",
        "import subprocess, time\n",
        "p = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.headless=true\"])\n",
        "time.sleep(3)  # give it a moment to boot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLwgfbhxG_EA",
        "outputId": "23306f03-4944-434c-a48c-66926adea400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-25T01:17:51+0000 lvl=warn msg=\"failed to open private leg\" id=93ad4f5160f7 privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-09-25T01:17:51+0000 lvl=warn msg=\"failed to open private leg\" id=607dbd017658 privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# close any old tunnels and agent\n",
        "for t in ngrok.get_tunnels():\n",
        "    ngrok.disconnect(t.public_url)\n",
        "ngrok.kill()\n",
        "\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(\"Streamlit app:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucnJDnGcHjhR",
        "outputId": "fad82d01-0f63-44c6-c81e-70d18d72a8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-25T01:17:54+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-aba35062-4b64-4cd0-87f2-7cd589213469 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app: NgrokTunnel: \"https://nonprominently-unpredisposing-burton.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}